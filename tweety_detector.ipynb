{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting Wav File to Save Space and Combine Folders Across Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data = \"/home/george-vengrovski/Documents/canary_song_detector/USA5207\"\n",
    "\n",
    "folders = os.listdir(root_data)\n",
    "\n",
    "if os.path.exists(root_data + \"/joined\") == False:\n",
    "    os.mkdir(root_data + \"/joined\")\n",
    "dst = root_data + \"/joined\"\n",
    "\n",
    "for folder in folders:\n",
    "    if folder == \"joined\":\n",
    "        continue \n",
    "\n",
    "    files = os.listdir(os.path.join(root_data, folder))\n",
    "    \n",
    "    for file in files:\n",
    "        if file.endswith('.wav'):\n",
    "            os.remove(os.path.join(root_data, folder, file))\n",
    "        else:\n",
    "            for matrix in os.listdir(os.path.join(root_data, folder, file)):\n",
    "                src = os.path.join(root_data, folder, file, matrix)\n",
    "                shutil.move(src, dst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test and Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"/home/george-vengrovski/Documents/canary_song_detector/USA5207/joined\"\n",
    "split = 0.8\n",
    "files = os.listdir(src)\n",
    "true_root_dir = \"/home/george-vengrovski/Documents/canary_song_detector\"\n",
    "train_dir = os.path.join(true_root_dir, \"train\")\n",
    "test_dir = os.path.join(true_root_dir, \"test\")\n",
    "\n",
    "if not os.path.exists(test_dir):\n",
    "    os.mkdir(test_dir)\n",
    "\n",
    "if not os.path.exists(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "\n",
    "for file in files:\n",
    "    x = np.random.uniform()\n",
    "    if x > split:\n",
    "        dest_dir = test_dir\n",
    "    else:\n",
    "        dest_dir = train_dir\n",
    "\n",
    "    dest_path = os.path.join(dest_dir, file)\n",
    "\n",
    "    # If destination path does not exist, move the file\n",
    "    if not os.path.exists(dest_path):\n",
    "        shutil.move(os.path.join(src, file), dest_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectorDataClass():\n",
    "    def __init__(self, dir, spec=513):\n",
    "        self.data = []\n",
    "        self.spec = spec\n",
    "\n",
    "        for file in os.listdir(dir):\n",
    "            self.data.append(os.path.join(dir, file))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        mat_data = scipy.io.loadmat(data)\n",
    "        mat_data = mat_data[\"song_data\"]\n",
    "        mat_data = mat_data[0][0]\n",
    "\n",
    "        arr1 = mat_data[0]\n",
    "        arr2 = mat_data[1]\n",
    "\n",
    "        # beware if spec shape changes, this might cause error\n",
    "        if arr1.shape[0] == self.spec:\n",
    "            spec = torch.Tensor(arr1)\n",
    "            raw_labels = torch.Tensor(arr2).int()\n",
    "        else:\n",
    "            spec = torch.Tensor(arr2)\n",
    "            raw_labels = torch.Tensor(arr1).int()\n",
    "\n",
    "        if raw_labels.shape == (0,0):\n",
    "            song = False\n",
    "        else:\n",
    "            song = True\n",
    "\n",
    "        # labels will be the same length as the song, but it will be filled with 1s between indcies of start and stops\n",
    "        labels = torch.zeros(size=(spec.shape[1],))\n",
    "\n",
    "        if song == True:\n",
    "            num_entries = raw_labels.shape[1]\n",
    "\n",
    "            for i in range(num_entries):\n",
    "                if i % 2 == 0:\n",
    "                    labels[raw_labels[i]:raw_labels[i+1]].fill_(1)\n",
    "        \n",
    "        spec = spec.unsqueeze(0)\n",
    "        \n",
    "        return spec, labels, song\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) \n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Extract sequences and labels\n",
    "    sequences = [x[0] for x in batch]\n",
    "    labels = [x[1] for x in batch]\n",
    "    \n",
    "    # Find the max length for padding\n",
    "    max_len_spec = max([s.size(2) for s in sequences])\n",
    "    max_len_labels = max([l.size(0) for l in labels])\n",
    "    \n",
    "    # Pad each sequence to max length\n",
    "    sequences_padded = []\n",
    "    for s in sequences:\n",
    "        pad_size = max_len_spec - s.size(2)\n",
    "        sequences_padded.append(F.pad(s, (0, pad_size)))\n",
    "    \n",
    "    # Pad each label to max length\n",
    "    labels_padded = []\n",
    "    for l in labels:\n",
    "        pad_size = max_len_labels - l.size(0)\n",
    "        labels_padded.append(F.pad(l, (0, pad_size)))\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    sequences_padded = torch.stack(sequences_padded)\n",
    "    labels_padded = torch.stack(labels_padded)\n",
    "    \n",
    "    # Get the song flags\n",
    "    songs = [x[2] for x in batch]\n",
    "    \n",
    "    return sequences_padded, labels_padded, songs\n",
    "\n",
    "train_dataset = DetectorDataClass(train_dir)\n",
    "test_dataset = DetectorDataClass(test_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class aviaBERT(nn.Module):\n",
    "    def __init__(self, d_transformer, nhead_transformer, embedding_dim, num_labels, tau=0.1, dropout=0.1, transformer_layers=1,dim_feedforward=256):\n",
    "        super(aviaBERT, self).__init__()\n",
    "        self.tau = tau\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # TweetyNet Front End\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(5, 5), stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(8, 1), stride=(8, 1))\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5, 5), stride=1, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(8, 1), stride=(8, 1))\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pos_conv1 = nn.Conv1d(d_transformer, d_transformer, kernel_size=3, padding=1, dilation=1)\n",
    "        self.pos_conv2 = nn.Conv1d(d_transformer, d_transformer, kernel_size=3, padding=2, dilation=2)\n",
    "\n",
    "        # transformer\n",
    "        self.transformerProjection = nn.Linear(512, d_transformer)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_transformer, nhead=nhead_transformer, batch_first=True, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=transformer_layers)\n",
    "        self.transformerDeProjection = nn.Linear(d_transformer, embedding_dim)\n",
    "\n",
    "        # label embedding\n",
    "        self.label_embedding = nn.Embedding(num_labels, embedding_dim)\n",
    "\n",
    "    def convolutional_positional_encoding(self, x):\n",
    "        pos = F.relu(self.pos_conv1(x))\n",
    "        pos = F.relu(self.pos_conv2(pos))\n",
    "        return pos\n",
    "\n",
    "    def feature_extractor_forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.flatten(1,2)\n",
    "        return x\n",
    "\n",
    "    def transformer_forward(self, x):\n",
    "        # project the input to the transformer dimension\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.transformerProjection(x)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        # add convolutional positional encoding\n",
    "        pos_enc = self.convolutional_positional_encoding(x)\n",
    "        x = x + pos_enc\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.transformerDeProjection(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor_forward(x)\n",
    "        x = self.transformer_forward(x)\n",
    "        return x\n",
    "\n",
    "    def BCE_loss(self, y_pred, y_true):\n",
    "        \"\"\"loss function for TweetyNet\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : torch.Tensor\n",
    "            output of TweetyNet model, shape (batch, classes, timebins)\n",
    "        y_true : torch.Tensor\n",
    "            one-hot encoded labels, shape (batch, classes, timebins)\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            mean cross entropy loss\n",
    "        \"\"\"\n",
    "        loss = torch.nn.BCEWithLogitsLoss()\n",
    "        return loss(input = y_pred, target = y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Trainable Parameters: 209,666\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detailed_count_parameters(model):\n",
    "    \"\"\"Print details of layers with the number of trainable parameters in the model.\"\"\"\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        total_params += param\n",
    "        # print(f\"Layer: {name} | Parameters: {param:,} | Shape: {list(parameter.shape)}\")\n",
    "    print(f\"\\nTotal Trainable Parameters: {total_params:,}\")\n",
    "\n",
    "epochs = 50\n",
    "learning_rate = 1e-4\n",
    "max_batches = 10  # maximum number of batches per epoch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "loss_list = []\n",
    "model = aviaBERT(d_transformer=64, nhead_transformer=2, embedding_dim=1, num_labels=1, tau=0.1, dropout=0.00)\n",
    "detailed_count_parameters(model)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Validation Loss: 4.46e-02, Initial Frame Error Rate: 98.46%\n",
      "Epoch [1/50], Training Loss: 5.22e-02, Validation Loss: 3.82e-02, Frame Error Rate: 98.69%\n",
      "Epoch [2/50], Training Loss: 6.16e-02, Validation Loss: 4.66e-02, Frame Error Rate: 98.54%\n",
      "Epoch [3/50], Training Loss: 5.47e-02, Validation Loss: 4.65e-02, Frame Error Rate: 98.48%\n",
      "Epoch [4/50], Training Loss: 6.48e-02, Validation Loss: 4.14e-02, Frame Error Rate: 98.46%\n",
      "Epoch [5/50], Training Loss: 6.66e-02, Validation Loss: 4.68e-02, Frame Error Rate: 98.93%\n",
      "Epoch [6/50], Training Loss: 6.30e-02, Validation Loss: 4.20e-02, Frame Error Rate: 98.21%\n",
      "Epoch [7/50], Training Loss: 6.05e-02, Validation Loss: 4.04e-02, Frame Error Rate: 98.79%\n",
      "Epoch [8/50], Training Loss: 5.85e-02, Validation Loss: 3.96e-02, Frame Error Rate: 98.92%\n",
      "Epoch [9/50], Training Loss: 5.31e-02, Validation Loss: 4.01e-02, Frame Error Rate: 98.58%\n",
      "Epoch [10/50], Training Loss: 6.52e-02, Validation Loss: 3.88e-02, Frame Error Rate: 99.24%\n",
      "Epoch [11/50], Training Loss: 6.87e-02, Validation Loss: 4.61e-02, Frame Error Rate: 97.71%\n",
      "Epoch [12/50], Training Loss: 6.82e-02, Validation Loss: 3.69e-02, Frame Error Rate: 98.30%\n",
      "Epoch [13/50], Training Loss: 5.55e-02, Validation Loss: 3.91e-02, Frame Error Rate: 98.19%\n",
      "Epoch [14/50], Training Loss: 6.00e-02, Validation Loss: 4.19e-02, Frame Error Rate: 98.21%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m avg_train_loss \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m num_batches\n\u001b[1;32m     71\u001b[0m loss_list\u001b[39m.\u001b[39mappend(avg_train_loss)\n\u001b[0;32m---> 73\u001b[0m avg_val_loss, avg_frame_error \u001b[39m=\u001b[39m validate_model(model, test_loader)\n\u001b[1;32m     74\u001b[0m val_loss_list\u001b[39m.\u001b[39mappend(avg_val_loss)\n\u001b[1;32m     75\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m], Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mavg_train_loss\u001b[39m:\u001b[39;00m\u001b[39m.2e\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Validation Loss: \u001b[39m\u001b[39m{\u001b[39;00mavg_val_loss\u001b[39m:\u001b[39;00m\u001b[39m.2e\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Frame Error Rate: \u001b[39m\u001b[39m{\u001b[39;00mavg_frame_error\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[70], line 24\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m     22\u001b[0m num_val_batches \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mfor\u001b[39;00m i, (spec, label, song) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(test_loader):\n\u001b[1;32m     25\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m \u001b[39m10\u001b[39m:\n\u001b[1;32m     26\u001b[0m           \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[66], line 11\u001b[0m, in \u001b[0;36mDetectorDataClass.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m     10\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index]\n\u001b[0;32m---> 11\u001b[0m     mat_data \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mloadmat(data)\n\u001b[1;32m     12\u001b[0m     mat_data \u001b[39m=\u001b[39m mat_data[\u001b[39m\"\u001b[39m\u001b[39msong_data\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     13\u001b[0m     mat_data \u001b[39m=\u001b[39m mat_data[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/scipy/io/matlab/_mio.py:227\u001b[0m, in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    226\u001b[0m     MR, _ \u001b[39m=\u001b[39m mat_reader_factory(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 227\u001b[0m     matfile_dict \u001b[39m=\u001b[39m MR\u001b[39m.\u001b[39mget_variables(variable_names)\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m mdict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m     mdict\u001b[39m.\u001b[39mupdate(matfile_dict)\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/scipy/io/matlab/_mio5.py:332\u001b[0m, in \u001b[0;36mMatFile5Reader.get_variables\u001b[0;34m(self, variable_names)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 332\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_var_array(hdr, process)\n\u001b[1;32m    333\u001b[0m \u001b[39mexcept\u001b[39;00m MatReadError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    334\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    335\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mUnreadable variable \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, because \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    336\u001b[0m         (name, err),\n\u001b[1;32m    337\u001b[0m         \u001b[39mWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/scipy/io/matlab/_mio5.py:292\u001b[0m, in \u001b[0;36mMatFile5Reader.read_var_array\u001b[0;34m(self, header, process)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_var_array\u001b[39m(\u001b[39mself\u001b[39m, header, process\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    276\u001b[0m \u001b[39m    \u001b[39m\u001b[39m''' Read array, given `header`\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \n\u001b[1;32m    278\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39m       `process`.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_matrix_reader\u001b[39m.\u001b[39marray_from_header(header, process)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def sum_squared_weights(model):\n",
    "    pass\n",
    "\n",
    "def frame_error_rate(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Compute the frame error rate.\n",
    "    y_pred: Tensor of shape (batch_size, time_steps) - typically the output of a softmax\n",
    "    y_true: Tensor of shape (batch_size, time_steps) - ground truth labels\n",
    "    Returns the frame error rate.\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = torch.round(y_pred)\n",
    "    mismatches = (y_pred != y_true)\n",
    "    error = mismatches.sum() / y_true.size(0) / y_true.size(1)\n",
    "    return error * 100  # return error as percentage\n",
    "\n",
    "# Modify the validation function to also compute and return the frame error rate\n",
    "def validate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_frame_error = 0\n",
    "    num_val_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (spec, label, song) in enumerate(test_loader):\n",
    "            if i > 10:\n",
    "              break\n",
    "            spec = spec.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            output = model.forward(spec)\n",
    "\n",
    "            loss = model.BCE_loss(y_pred = output.squeeze(2), y_true=label)\n",
    "\n",
    "            total_frame_error += frame_error_rate(y_pred = output.squeeze(2), y_true=label).item()\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            num_val_batches += 1\n",
    "\n",
    "    return total_val_loss / num_val_batches, total_frame_error / num_val_batches\n",
    "\n",
    "initial_val_loss, initial_frame_error = validate_model(model, test_loader)\n",
    "print(f'Initial Validation Loss: {initial_val_loss:.2e}, Initial Frame Error Rate: {initial_frame_error:.2f}%')\n",
    "\n",
    "loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Training Loop\n",
    "    model.train()\n",
    "    for i, (spec, label, song) in enumerate(train_loader):\n",
    "        if i > 10:\n",
    "          break\n",
    "        spec = spec.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = model.forward(spec)\n",
    "\n",
    "        loss = model.BCE_loss(y_pred = output.squeeze(2), y_true=label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    loss_list.append(avg_train_loss)\n",
    "\n",
    "    avg_val_loss, avg_frame_error = validate_model(model, test_loader)\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {avg_train_loss:.2e}, Validation Loss: {avg_val_loss:.2e}, Frame Error Rate: {avg_frame_error:.2f}%')\n",
    "\n",
    "# print loss curve\n",
    "plt.plot(loss_list, label='Training Loss')\n",
    "plt.plot(val_loss_list, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"final loss {loss_list[-1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canary-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
