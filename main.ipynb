{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms \n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.signal import spectrogram\n",
    "from scipy.fftpack import dst, dct\n",
    "\n",
    "songs = '/home/george-vengrovski/Documents/canary_song_detector/sample_songs'\n",
    "not_songs = '/home/george-vengrovski/Documents/canary_song_detector/sample_not_songs'\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "def train_test_split(train_split=0.8):\n",
    "    # Create directories if not already created\n",
    "    if not os.path.exists('test'):\n",
    "        os.makedirs('test')\n",
    "\n",
    "    if not os.path.exists('train'):\n",
    "        os.makedirs('train')\n",
    "\n",
    "    # Get a list of all song files and non-song files\n",
    "    songs = glob.glob('/home/george-vengrovski/Documents/canary_song_detector/sample_songs/*.wav')\n",
    "    not_songs = glob.glob('/home/george-vengrovski/Documents/canary_song_detector/sample_not_songs/*.wav')\n",
    "\n",
    "    # Split songs into train and test sets\n",
    "    train_songs = songs[:int(len(songs) * train_split)]\n",
    "    test_songs = songs[int(len(songs) * train_split):]\n",
    "\n",
    "    # Copy song files into train and test directories with updated filenames\n",
    "    for song in train_songs:\n",
    "        file_name = os.path.basename(song)\n",
    "        new_file_name = os.path.splitext(file_name)[0] + '_song.wav'\n",
    "        shutil.copy(song, os.path.join('train', new_file_name))\n",
    "\n",
    "    for song in test_songs:\n",
    "        file_name = os.path.basename(song)\n",
    "        new_file_name = os.path.splitext(file_name)[0] + '_song.wav'\n",
    "        shutil.copy(song, os.path.join('test', new_file_name))\n",
    "\n",
    "    # Split non-song files into train and test sets\n",
    "    train_not_songs = not_songs[:int(len(not_songs) * train_split)]\n",
    "    test_not_songs = not_songs[int(len(not_songs) * train_split):]\n",
    "\n",
    "    # Copy non-song files into train and test directories with updated filenames\n",
    "    for not_song in train_not_songs:\n",
    "        file_name = os.path.basename(not_song)\n",
    "        new_file_name = os.path.splitext(file_name)[0] + '_not_song.wav'\n",
    "        shutil.copy(not_song, os.path.join('train', new_file_name))\n",
    "\n",
    "    for not_song in test_not_songs:\n",
    "        file_name = os.path.basename(not_song)\n",
    "        new_file_name = os.path.splitext(file_name)[0] + '_not_song.wav'\n",
    "        shutil.copy(not_song, os.path.join('test', new_file_name))\n",
    "        \n",
    "train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio.transforms as T\n",
    "import random\n",
    "\n",
    "class DataSet(Dataset):\n",
    "    def __init__(self, type):\n",
    "        song_files = []\n",
    "        not_song_files = []\n",
    "\n",
    "        folder = 'train' if type == 'train' else 'test' \n",
    "\n",
    "        for file in glob.glob(folder + '/*.wav'):\n",
    "            if 'not_song' in file:\n",
    "                not_song_files.append(file)\n",
    "            else:\n",
    "                song_files.append(file)\n",
    "\n",
    "        # Combine all the files without sampling\n",
    "        self.file_paths = song_files + not_song_files\n",
    "        self.labels = [1]*len(song_files) + [0]*len(not_song_files)\n",
    "\n",
    "        # Shuffle file_paths and labels together\n",
    "        combined = list(zip(self.file_paths, self.labels))\n",
    "        random.shuffle(combined)\n",
    "        self.file_paths, self.labels = zip(*combined)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.file_paths[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        mean = waveform.mean()\n",
    "        std = waveform.std()\n",
    "        waveform = (waveform - mean) / std\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.float32)  # labels are converted to a float tensor here\n",
    "\n",
    "        return waveform, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        waveforms, labels = zip(*batch)\n",
    "\n",
    "        # Pad the waveforms\n",
    "        waveforms = pad_sequence([torch.flatten(w) for w in waveforms], batch_first=True)\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)  # labels should be a tensor, not a tuple\n",
    "        \n",
    "        return waveforms, labels\n",
    "\n",
    "    \n",
    "    def visualize_spectrogram(self, index):\n",
    "        file_path = self.file_paths[index]\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "        # Create a spectrogram from the waveform\n",
    "        spectrogram_transform = T.Spectrogram()\n",
    "        spectrogram = spectrogram_transform(waveform)\n",
    "\n",
    "        # Remove the channel dimension\n",
    "        spectrogram = spectrogram.squeeze(0)\n",
    "\n",
    "        # Plot the spectrogram\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.imshow(spectrogram.log2(), aspect='auto', origin='lower')\n",
    "        plt.title('Spectrogram of ' + file_path)\n",
    "        plt.xlabel('Frames')\n",
    "        plt.ylabel('Frequency bins')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = DataSet(type='train')\n",
    "train_dataset.visualize_spectrogram(0)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "test_dataset = DataSet(type='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier Architecture \n",
    "- 4 1d conv layers for the waveform of the periodicities, and amplitude \n",
    "- LSTM layers afterwards \n",
    "- Fully connected layer with two nodes for song v non-song "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_extractor_dim, hidden_size, num_layers):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        # Feature extractor\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=feature_extractor_dim, kernel_size=10, padding=1, stride=5)\n",
    "        self.conv2 = nn.Conv1d(in_channels=feature_extractor_dim, out_channels=feature_extractor_dim, kernel_size=4, padding=1, stride=4)\n",
    "        self.conv3 = nn.Conv1d(in_channels=feature_extractor_dim, out_channels=feature_extractor_dim, kernel_size=4, padding=1, stride=4)\n",
    "        self.conv4 = nn.Conv1d(in_channels=feature_extractor_dim, out_channels=feature_extractor_dim, kernel_size=4, padding=1, stride=4)\n",
    "        self.conv5 = nn.Conv1d(in_channels=feature_extractor_dim, out_channels=feature_extractor_dim, kernel_size=4, padding=1, stride=4)\n",
    "        self.conv6 = nn.Conv1d(in_channels=feature_extractor_dim, out_channels=feature_extractor_dim, kernel_size=3, padding=1, stride=3)\n",
    "        self.conv7 = nn.Conv1d(in_channels=feature_extractor_dim, out_channels=feature_extractor_dim, kernel_size=3, padding=1, stride=3)\n",
    "\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size=feature_extractor_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Fully connected layer \n",
    "        self.fc1 = nn.Linear(hidden_size*2, 1)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "\n",
    "        # Reshape for GRU\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # x is the output for each timestep of the GRU while h is the final hidden state\n",
    "        x, h = self.gru(x)\n",
    "\n",
    "        # squish the 2 directions into 1\n",
    "\n",
    "        x = h.permute(1, 0, 2)\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        x = self.fc1(F.relu(x))\n",
    "        # x = self.fc2(F.relu(x))\n",
    "        # x = self.fc3(F.relu(x))\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        x = x.squeeze(0)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, labels = next(iter(train_loader))\n",
    "\n",
    "waveform = waveform.unsqueeze(1)\n",
    "\n",
    "model = Classifier(feature_extractor_dim=64, hidden_size=200, num_layers=1)\n",
    "print(model.forward(waveform).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.0001\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Classifier(feature_extractor_dim=16, hidden_size=64, num_layers=1).to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    for waveform, label in train_loader:\n",
    "        waveform = waveform.unsqueeze(1).to(device)\n",
    "        label = label.to(device)  # Changed here\n",
    "        output = model(waveform)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted = (output > 0.5).float()\n",
    "        train_preds.extend(predicted.detach().cpu().numpy())\n",
    "        train_labels.extend(label.detach().cpu().numpy())\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()  # compute the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    train_accuracy = balanced_accuracy_score(train_labels, train_preds) * 100\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print ('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.2f}%'.format(epoch+1, epochs, train_loss / len(train_loader), train_accuracy))\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    with torch.no_grad():\n",
    "        for waveform, label in test_loader:\n",
    "            waveform = waveform.unsqueeze(1).to(device)\n",
    "            label = torch.tensor(label, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            output = model(waveform)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = (output > 0.5).float()\n",
    "            test_preds.extend(predicted.cpu().numpy())\n",
    "            test_labels.extend(label.cpu().numpy())\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, label)\n",
    "            test_loss += loss.item()\n",
    "        \n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    test_accuracy = balanced_accuracy_score(test_labels, test_preds) * 100\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    print('Epoch [{}/{}], Test Loss: {:.4f}, Test Accuracy: {:.2f}%'.format(epoch+1, epochs, test_loss / len(test_loader), test_accuracy))\n",
    "\n",
    "# Plot the training and test losses over time\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training and Test Loss\")\n",
    "plt.plot(train_losses,label=\"train\")\n",
    "plt.plot(test_losses,label=\"test\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and test accuracies over time\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training and Test Accuracy\")\n",
    "plt.plot(train_accuracies,label=\"train\")\n",
    "plt.plot(test_accuracies,label=\"test\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the weights\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "iterations = 1000\n",
    "learning_rate = .1\n",
    "\n",
    "length = 44100 * 1 \n",
    "channel = 0 \n",
    "\n",
    "# Create a random tensor and make it require gradients\n",
    "random_tensor = torch.randn(1, 1, length).to(device)\n",
    "random_tensor.requires_grad = True\n",
    "# Create an optimizer\n",
    "optimizer = optim.Adam([random_tensor], lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "for i in range(iterations):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass up to the first convolutional layer\n",
    "    output = model(random_tensor)\n",
    "    # output = F.relu(output[0].flatten(start_dim=1))\n",
    "\n",
    "    # Select the activation of the first channel\n",
    "    activation = output[0]\n",
    "\n",
    "    # We want to maximize the activation, so we minimize the negative activation\n",
    "    loss = -activation.mean()\n",
    "\n",
    "    print(loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the input tensor\n",
    "    optimizer.step()\n",
    "\n",
    "print(random_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlibrosa\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Convert the tensor back to numpy and cpu for plotting\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m random_tensor_np \u001b[39m=\u001b[39m random_tensor\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m \u001b[39m# As your tensor is likely a 3D tensor (batch_size, channels, length), \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# you might want to select one sample for visualization\u001b[39;00m\n\u001b[1;32m     10\u001b[0m selected_sample \u001b[39m=\u001b[39m random_tensor_np[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, :]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Convert the tensor back to numpy and cpu for plotting\n",
    "random_tensor_np = random_tensor.detach().cpu().numpy()\n",
    "\n",
    "# As your tensor is likely a 3D tensor (batch_size, channels, length), \n",
    "# you might want to select one sample for visualization\n",
    "selected_sample = random_tensor_np[0, 0, :]\n",
    "\n",
    "# Compute the spectrogram\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(selected_sample)), ref=np.max)\n",
    "\n",
    "# Display the spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(D, sr=44100, x_axis='time', y_axis='log')\n",
    "plt.colorbar(format='%+2.0f Db')\n",
    "plt.title('Spectrogram')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canary-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
