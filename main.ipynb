{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms \n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.signal import spectrogram\n",
    "from scipy.fftpack import dst, dct\n",
    "\n",
    "songs = '/home/george-vengrovski/Documents/canary_song_detector/sample_songs'\n",
    "not_songs = '/home/george-vengrovski/Documents/canary_song_detector/sample_not_songs'\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "def train_test_split(train_split=0.8):\n",
    "    # Create directories if not already created\n",
    "    if not os.path.exists('test'):\n",
    "        os.makedirs('test')\n",
    "\n",
    "    if not os.path.exists('train'):\n",
    "        os.makedirs('train')\n",
    "\n",
    "    # Get a list of all song files and non-song files\n",
    "    songs = glob.glob('/home/george-vengrovski/Documents/canary_song_detector/sample_songs/*.wav')\n",
    "    not_songs = glob.glob('/home/george-vengrovski/Documents/canary_song_detector/sample_not_songs/*.wav')\n",
    "\n",
    "    # Split songs into train and test sets\n",
    "    train_songs = songs[:int(len(songs) * train_split)]\n",
    "    test_songs = songs[int(len(songs) * train_split):]\n",
    "\n",
    "    # Copy song files into train and test directories with updated filenames\n",
    "    for song in train_songs:\n",
    "        file_name = os.path.basename(song)\n",
    "        new_file_name = os.path.splitext(file_name)[0] + '_song.wav'\n",
    "        shutil.copy(song, os.path.join('train', new_file_name))\n",
    "\n",
    "    for song in test_songs:\n",
    "        file_name = os.path.basename(song)\n",
    "        new_file_name = os.path.splitext(file_name)[0] + '_song.wav'\n",
    "        shutil.copy(song, os.path.join('test', new_file_name))\n",
    "\n",
    "    # Split non-song files into train and test sets\n",
    "    train_not_songs = not_songs[:int(len(not_songs) * train_split)]\n",
    "    test_not_songs = not_songs[int(len(not_songs) * train_split):]\n",
    "\n",
    "    # Copy non-song files into train and test directories with updated filenames\n",
    "    for not_song in train_not_songs:\n",
    "        file_name = os.path.basename(not_song)\n",
    "        new_file_name = os.path.splitext(file_name)[0] + '_not_song.wav'\n",
    "        shutil.copy(not_song, os.path.join('train', new_file_name))\n",
    "\n",
    "    for not_song in test_not_songs:\n",
    "        file_name = os.path.basename(not_song)\n",
    "        new_file_name = os.path.splitext(file_name)[0] + '_not_song.wav'\n",
    "        shutil.copy(not_song, os.path.join('test', new_file_name))\n",
    "        \n",
    "train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataSet' object has no attribute 'file_paths'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 75\u001b[0m\n\u001b[1;32m     70\u001b[0m         plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     74\u001b[0m train_dataset \u001b[39m=\u001b[39m DataSet(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m train_dataset\u001b[39m.\u001b[39;49mvisualize_spectrogram(\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     76\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, collate_fn\u001b[39m=\u001b[39mtrain_dataset\u001b[39m.\u001b[39mcollate_fn)\n\u001b[1;32m     78\u001b[0m test_dataset \u001b[39m=\u001b[39m DataSet(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[72], line 52\u001b[0m, in \u001b[0;36mDataSet.visualize_spectrogram\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvisualize_spectrogram\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m---> 52\u001b[0m     file_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_paths[index]\n\u001b[1;32m     53\u001b[0m     waveform, sample_rate \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mload(file_path)\n\u001b[1;32m     55\u001b[0m     \u001b[39m# Create a spectrogram from the waveform\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataSet' object has no attribute 'file_paths'"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio.transforms as T\n",
    "import random\n",
    "\n",
    "class DataSet(Dataset):\n",
    "    def __init__(self, type):\n",
    "        song_files = []\n",
    "        not_song_files = []\n",
    "\n",
    "        folder = 'train' if type == 'train' else 'test' \n",
    "\n",
    "        for file in glob.glob(folder + '/*.wav'):\n",
    "            if 'not_song' in file:\n",
    "                not_song_files.append(file)\n",
    "            else:\n",
    "                song_files.append(file)\n",
    "\n",
    "        # Combine all the files without sampling\n",
    "        self.file_paths = song_files + not_song_files\n",
    "        self.labels = [1]*len(song_files) + [0]*len(not_song_files)\n",
    "\n",
    "        # Shuffle file_paths and labels together\n",
    "        combined = list(zip(self.file_paths, self.labels))\n",
    "        random.shuffle(combined)\n",
    "        self.file_paths, self.labels = zip(*combined)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.file_paths[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        mean = waveform.mean()\n",
    "        std = waveform.std()\n",
    "        waveform = (waveform - mean) / std\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.float32)  # labels are converted to a float tensor here\n",
    "\n",
    "        return waveform, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        waveforms, labels = zip(*batch)\n",
    "\n",
    "        # Pad the waveforms\n",
    "        waveforms = pad_sequence([torch.flatten(w) for w in waveforms], batch_first=True)\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)  # labels should be a tensor, not a tuple\n",
    "        \n",
    "        return waveforms, labels\n",
    "\n",
    "    \n",
    "    def visualize_spectrogram(self, index):\n",
    "        file_path = self.file_paths[index]\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "        # Create a spectrogram from the waveform\n",
    "        spectrogram_transform = T.Spectrogram()\n",
    "        spectrogram = spectrogram_transform(waveform)\n",
    "\n",
    "        # Remove the channel dimension\n",
    "        spectrogram = spectrogram.squeeze(0)\n",
    "\n",
    "        # Plot the spectrogram\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.imshow(spectrogram.log2(), aspect='auto', origin='lower')\n",
    "        plt.title('Spectrogram of ' + file_path)\n",
    "        plt.xlabel('Frames')\n",
    "        plt.ylabel('Frequency bins')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = DataSet(type='train')\n",
    "train_dataset.visualize_spectrogram(0)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "test_dataset = DataSet(type='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier Architecture \n",
    "- 4 1d conv layers for the waveform of the periodicities, and amplitude \n",
    "- LSTM layers afterwards \n",
    "- Fully connected layer with two nodes for song v non-song "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_extractor_dim, hidden_size, num_layers):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        # Feature extractor\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=feature_extractor_dim, kernel_size=10, padding=1, stride=5)\n",
    "        self.conv2 = nn.Conv1d(in_channels=feature_extractor_dim, out_channels=feature_extractor_dim, kernel_size=4, padding=1, stride=4)\n",
    "        self.conv3 = nn.Conv1d(in_channels=feature_extractor_dim, out_channels=feature_extractor_dim, kernel_size=4, padding=1, stride=4)\n",
    "        self.conv4 = nn.Conv1d(in_channels=feature_extractor_dim, out_channels=feature_extractor_dim, kernel_size=4, padding=1, stride=4)\n",
    "        self.conv5 = nn.Conv1d(in_channels=feature_extractor_dim, out_channels=feature_extractor_dim, kernel_size=4, padding=1, stride=4)\n",
    "        self.conv6 = nn.Conv1d(in_channels=feature_extractor_dim, out_channels=feature_extractor_dim, kernel_size=3, padding=1, stride=3)\n",
    "        self.conv7 = nn.Conv1d(in_channels=feature_extractor_dim, out_channels=feature_extractor_dim, kernel_size=3, padding=1, stride=3)\n",
    "\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size=feature_extractor_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Fully connected layer \n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "\n",
    "        # Reshape for GRU\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # x is the output for each timestep of the GRU while h is the final hidden state\n",
    "        x, h = self.gru(x)\n",
    "\n",
    "        # squish the 2 directions into 1\n",
    "\n",
    "        x = h.permute(1, 0, 2)\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        x = self.fc1(F.relu(x))\n",
    "        x = self.fc2(F.relu(x))\n",
    "        x = self.fc3(F.relu(x))\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        x = x.squeeze(0)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 119])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "waveform, labels = next(iter(train_loader))\n",
    "\n",
    "# # switch first and second dim\n",
    "# waveform = waveform.transpose(1,2)\n",
    "\n",
    "waveform = waveform.unsqueeze(1)\n",
    "\n",
    "model = Classifier(feature_extractor_dim=64, hidden_size=200, num_layers=1)\n",
    "print(model.forward(waveform).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,582,273 trainable parameters\n",
      "Epoch [1/5], Train Loss: 0.6699, Train Accuracy: 52.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4746/182769482.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label, dtype=torch.float32).unsqueeze(1).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Test Loss: 0.6212, Test Accuracy: 85.19%\n",
      "Epoch [2/5], Train Loss: 0.6777, Train Accuracy: 60.48%\n",
      "Epoch [2/5], Test Loss: 0.6451, Test Accuracy: 50.00%\n",
      "Epoch [3/5], Train Loss: 0.4530, Train Accuracy: 90.00%\n",
      "Epoch [3/5], Test Loss: 0.5902, Test Accuracy: 50.00%\n",
      "Epoch [4/5], Train Loss: 0.3196, Train Accuracy: 90.00%\n",
      "Epoch [4/5], Test Loss: 0.6295, Test Accuracy: 50.00%\n",
      "Epoch [5/5], Train Loss: 0.2372, Train Accuracy: 92.38%\n",
      "Epoch [5/5], Test Loss: 0.4183, Test Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "learning_rate = 0.0001\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Classifier(feature_extractor_dim=256, hidden_size=64, num_layers=1).to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for waveform, label in train_loader:\n",
    "        waveform = waveform.unsqueeze(1).to(device)\n",
    "        label = label.to(device)  # Changed here\n",
    "        output = model(waveform)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted = (output > 0.5).float()\n",
    "        train_total += label.size(0)\n",
    "        train_correct += (predicted == label).sum().item()\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()  # compute the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    print ('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.2f}%'.format(epoch+1, epochs, train_loss / len(train_loader), train_accuracy))\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for waveform, label in test_loader:\n",
    "            waveform = waveform.unsqueeze(1).to(device)\n",
    "            label = torch.tensor(label, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            output = model(waveform)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = (output > 0.5).float()\n",
    "            test_total += label.size(0)\n",
    "            test_correct += (predicted == label).sum().item()\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, label)\n",
    "            test_loss += loss.item()\n",
    "        \n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    print('Epoch [{}/{}], Test Loss: {:.4f}, Test Accuracy: {:.2f}%'.format(\n",
    "        epoch+1, epochs, test_loss / len(test_loader), test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(1.),)\n",
      "tensor([1.0000], device='cuda:0', grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "waveform, labels = next(iter(train_loader))\n",
    "print(labels)\n",
    "waveform = waveform.unsqueeze(1).to(device)\n",
    "print(model.forward(waveform))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canary-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
