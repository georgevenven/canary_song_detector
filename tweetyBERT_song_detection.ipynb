{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import shutil\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn import cluster\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "\n",
    "data_root = \"/home/george-vengrovski/Documents/TweetyBERT/llb3_data_matrices\"\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psuedo_label_generation\n",
    "\n",
    "data_root = \"/home/george-vengrovski/Documents/canary_song_detector/data\"\n",
    "train = \"/home/george-vengrovski/Documents/canary_song_detector/train\"\n",
    "test = \"/home/george-vengrovski/Documents/canary_song_detector/test\"\n",
    "\n",
    "processor = psuedo_label_generation.SpectrogramProcessor(data_root=data_root, train_dir=train, test_dir=test, n_clusters=100)\n",
    "\n",
    "### CAREFUL\n",
    "processor.clear_directory(train)\n",
    "processor.clear_directory(test)\n",
    "### CAREFUL \n",
    "\n",
    "processor.generate_train_test()\n",
    "# this is 5,000 samples * num times in samples (10 in the case of 100 timebins in 1000 timebin segment)\n",
    "processor.generate_embedding(samples=5e3, time_bins_per_sample=100, reduction_dims=2)\n",
    "processor.plot_embedding_and_labels()\n",
    "processor.generate_train_test_labels(reduce=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_class import SongDataSet_Image, plot_spectrogram_and_labels\n",
    "\n",
    "train_dir = \"train\"\n",
    "test_dir = \"test\"\n",
    "\n",
    "train_dataset = SongDataSet_Image(train_dir)\n",
    "test_dataset = SongDataSet_Image(test_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=24, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Get a batch of data\n",
    "spec, psuedo_labels, ground_truth_labels = next(iter(train_loader))\n",
    "\n",
    "# Plotting\n",
    "spec = spec[0].squeeze(0)  # If your data is batched, get the first item\n",
    "plot_spectrogram_and_labels(spec, ground_truth_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "from model import TweetyBERT\n",
    "\n",
    "def detailed_count_parameters(model):\n",
    "    \"\"\"Print details of layers with the number of trainable parameters in the model.\"\"\"\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        total_params += param\n",
    "        # print(f\"Layer: {name} | Parameters: {param:,} | Shape: {list(parameter.shape)}\")\n",
    "    print(f\"\\nTotal Trainable Parameters: {total_params:,}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "model = TweetyBERT(d_transformer=64, nhead_transformer=2, embedding_dim=100, num_labels=100, tau=1, dropout=0.1, dim_feedforward=64, transformer_layers=2, reduced_embedding= 27)\n",
    "detailed_count_parameters(model)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import ModelTrainer\n",
    "\n",
    "# Usage:\n",
    "trainer = ModelTrainer(model, train_loader, test_loader, optimizer, device, max_steps=1001, eval_interval=5e1, save_interval=500)\n",
    "trainer.train()\n",
    "trainer.plot_results()\n",
    "print(f\"final loss {trainer.loss_list[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "predictions_arr = []\n",
    "psuedo_labels_arr = []\n",
    "ground_truth_labels_arr =[]\n",
    "spec_arr = []\n",
    "layers_arr = []\n",
    "batch_limit = 20\n",
    "k_means_batch_limit = 10\n",
    "activations = []\n",
    "k_means_arr = []\n",
    "cluster_labels_arr = []\n",
    "\n",
    "def one_hot_encode(labels, num_classes=256):\n",
    "    one_hot = np.zeros((labels.shape[0], num_classes))\n",
    "    one_hot[np.arange(labels.shape[0]), labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "for i, (data, psuedo_labels, ground_truth_label) in enumerate(train_loader):\n",
    "    if i > k_means_batch_limit:\n",
    "        break\n",
    "    predictions, layers = model.inference_forward(data.to(device), reduced_embedding=False)\n",
    "    temp = predictions\n",
    "    temp = temp.reshape(temp.shape[0] * temp.shape[1], temp.shape[2])\n",
    "    activations.append(temp.detach().cpu().numpy())\n",
    "\n",
    "k_means_arr = np.concatenate(activations, axis=0)\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "kmeans.fit(k_means_arr)\n",
    "\n",
    "\n",
    "cluster_labels_arr = []\n",
    "k_means_labels = []\n",
    "\n",
    "for i, (data, psuedo_labels, ground_truth_label) in enumerate(train_loader):\n",
    "    if i > batch_limit:\n",
    "      break\n",
    "\n",
    "    predictions, layers = model.inference_forward(data.to(device), reduced_embedding=False)\n",
    "    temp = predictions\n",
    "    predictions = predictions\n",
    "    k_means_labels = temp\n",
    "    # becomes batch + time, features \n",
    "    k_means_labels = k_means_labels.reshape(temp.shape[0] * temp.shape[1], temp.shape[2])\n",
    "\n",
    "    k_means_labels = kmeans.predict(k_means_labels.detach().cpu().numpy())\n",
    "    k_means_labels = one_hot_encode(k_means_labels, num_classes = temp.shape[2])\n",
    "\n",
    "    # becomes batch+time, feature label\n",
    "\n",
    "    k_means_labels = k_means_labels.reshape(temp.shape[0] * 10, 100, temp.shape[2])\n",
    "    k_means_labels = torch.Tensor(k_means_labels)\n",
    "    k_means_labels = torch.argmax(k_means_labels, dim=-1)\n",
    "    k_means_labels = k_means_labels.squeeze(1)\n",
    "\n",
    "    spec = data.squeeze(1).permute(0,2,1)\n",
    "    predictions = predictions.reshape(predictions.shape[0] * 10, 100, predictions.shape[2])\n",
    "    spec = spec.reshape(spec.shape[0] * 10, 100, spec.shape[2])\n",
    "    psuedo_labels = psuedo_labels.reshape(psuedo_labels.shape[0] * 10, 100, psuedo_labels.shape[2])\n",
    "    ground_truth_label = ground_truth_label.reshape(ground_truth_label.shape[0] * 10, 100, ground_truth_label.shape[2])\n",
    "    predictions = predictions.flatten(1,2)\n",
    "\n",
    "    spec = spec.flatten(1,2)\n",
    "    psuedo_labels = torch.argmax(psuedo_labels, dim=-1)\n",
    "    psuedo_labels = psuedo_labels.squeeze(1)\n",
    "    ground_truth_label = torch.argmax(ground_truth_label, dim=-1)\n",
    "    ground_truth_label = ground_truth_label.squeeze(1)\n",
    "\n",
    "    spec_arr.append(spec.detach().cpu().numpy())\n",
    "    predictions_arr.append(predictions.detach().cpu().numpy())\n",
    "    ground_truth_labels_arr.append(ground_truth_label.cpu().numpy())\n",
    "    psuedo_labels_arr.append(psuedo_labels.cpu().numpy())\n",
    "    cluster_labels_arr.append(k_means_labels.cpu().numpy())\n",
    "\n",
    "\n",
    "np.save(\"ground_truth_arr\", ground_truth_labels_arr)\n",
    "np.save(\"predictions_arr\", predictions_arr)\n",
    "np.save(\"psuedo_labels_arr\", psuedo_labels_arr)\n",
    "np.save(\"spec_arr\", spec_arr)\n",
    "np.save(\"cluster_labels_arr\", cluster_labels_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import cm\n",
    "\n",
    "# Load the color map data from the pickle file\n",
    "file_path = '/home/george-vengrovski/Documents/TweetyBERT/category_colors.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    color_map_data = pickle.load(file)\n",
    "\n",
    "# Convert your color_map_data values to a format suitable for matplotlib\n",
    "label_to_color = {label: tuple(color) for label, color in color_map_data.items()}\n",
    "\n",
    "# Load prediction and ground truth labels\n",
    "predictions = np.load(\"predictions_arr.npy\", allow_pickle=True)\n",
    "ground_truth_labels = np.load(\"ground_truth_arr.npy\", allow_pickle=True)\n",
    "cluster_labels = np.load(\"cluster_labels_arr.npy\", allow_pickle=True)\n",
    "\n",
    "# Concatenate arrays\n",
    "predictions = np.concatenate([prediction for prediction in predictions], axis=0)\n",
    "ground_truth_labels = np.concatenate([label for label in ground_truth_labels], axis=0)\n",
    "cluster_labels = np.concatenate([label for label in cluster_labels], axis=0)\n",
    "\n",
    "# Calculate the number of unique cluster labels\n",
    "num_unique_clusters = len(np.unique(cluster_labels))\n",
    "\n",
    "# Generate a color map\n",
    "new_cmap = cm.get_cmap('jet', num_unique_clusters)\n",
    "\n",
    "# Create a dictionary mapping cluster label to color\n",
    "cluster_to_color = {label: new_cmap(i / num_unique_clusters) for i, label in enumerate(np.unique(cluster_labels))}\n",
    "print(cluster_to_color)\n",
    "\n",
    "\n",
    "print(f\"labels shape{ground_truth_labels.shape}\")\n",
    "print(f\"outputs shape{predictions.shape}\")\n",
    "print(f\"clusters shape{cluster_labels.shape}\")\n",
    "\n",
    "colors_for_points = []\n",
    "colors_for_clusters = []  # New list for K-means labels\n",
    "\n",
    "# Prepare colors for ground truth labels\n",
    "for label_row in ground_truth_labels:\n",
    "    row_colors = [label_to_color[int(lbl)] for lbl in label_row]\n",
    "    avg_color = np.mean(row_colors, axis=0)\n",
    "    colors_for_points.append(avg_color)\n",
    "colors_for_clusters = []  # New list for K-means labels\n",
    "\n",
    "# Prepare colors for K-means labels\n",
    "for label_row in cluster_labels:\n",
    "    row_colors = [cluster_to_color[int(lbl)] for lbl in label_row]\n",
    "    avg_color = np.mean(row_colors, axis=0)\n",
    "    colors_for_clusters.append(avg_color)\n",
    "\n",
    "colors_for_clusters = np.array(colors_for_clusters)  # New array for K-means labels\n",
    "\n",
    "# Initialize UMAP reducer\n",
    "reducer = umap.UMAP(random_state=42, n_neighbors=20, min_dist=0.1, n_components=2, metric='euclidean')\n",
    "\n",
    "# UMAP projection of the TweetyBERT predictions\n",
    "embedding_outputs = reducer.fit_transform(predictions)\n",
    "\n",
    "# Plotting the UMAP projection for ground truth labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(embedding_outputs[:, 0], embedding_outputs[:, 1], s=5, c=colors_for_points)\n",
    "plt.title('UMAP projection of the TweetyBERT (Ground Truth)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the UMAP projection for K-means labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(embedding_outputs[:, 0], embedding_outputs[:, 1], s=5, c=colors_for_clusters)\n",
    "plt.title('UMAP projection of the TweetyBERT (K-means Labels)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "\n",
    "predictions_arr = []\n",
    "psuedo_labels_arr = []\n",
    "ground_truth_labels_arr =[]\n",
    "spec_arr = []\n",
    "layers_arr = []\n",
    "batch_limit = 20\n",
    "k_means_batch_limit = 2\n",
    "activations = []\n",
    "k_means_arr = []\n",
    "cluster_labels_arr = []\n",
    "\n",
    "for i, (data, psuedo_labels, ground_truth_label) in enumerate(train_loader):\n",
    "    if i > batch_limit:\n",
    "      break\n",
    "\n",
    "    predictions, layers = model.inference_forward(data.to(device), reduced_embedding=False)\n",
    "    temp = predictions\n",
    "    k_means_labels = temp\n",
    "    # becomes batch + time, features \n",
    "    k_means_labels = k_means_labels.reshape(temp.shape[0] * temp.shape[1], temp.shape[2])\n",
    "\n",
    "    k_means_labels = kmeans.predict(k_means_labels.detach().cpu().numpy())\n",
    "    k_means_labels = one_hot_encode(k_means_labels, num_classes = temp.shape[2])\n",
    "\n",
    "    # becomes batch+time, feature label\n",
    "    k_means_labels = k_means_labels.reshape(temp.shape[0] * temp.shape[1], temp.shape[2])\n",
    "\n",
    "    k_means_labels = torch.Tensor(k_means_labels)\n",
    "    k_means_labels = torch.argmax(k_means_labels, dim=-1)\n",
    "\n",
    "    ground_truth_label = ground_truth_label.reshape(ground_truth_label.shape[0] * ground_truth_label.shape[1], ground_truth_label.shape[2])\n",
    "    ground_truth_label = torch.argmax(ground_truth_label, dim=-1)\n",
    "\n",
    "    ground_truth_labels_arr.append(ground_truth_label.cpu().numpy())\n",
    "    cluster_labels_arr.append(k_means_labels.cpu().numpy())\n",
    "    \n",
    "ground_truth_labels = np.concatenate([label for label in ground_truth_labels_arr], axis=0)\n",
    "cluster_labels = np.concatenate([label for label in cluster_labels_arr], axis=0)\n",
    "\n",
    "# cluster labels merge\n",
    "cluster_labels[(cluster_labels == 0)] = 0\n",
    "cluster_labels[(cluster_labels == 1)] = 1\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming ground_truth_labels and cluster_labels are NumPy arrays\n",
    "f1 = f1_score(ground_truth_labels, cluster_labels, average='weighted')\n",
    "\n",
    "print(f'F1 Score: {f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canary-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
